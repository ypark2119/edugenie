{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”‘ Loaded Gemini key: AIzaSyBjNSgnILHbnsLM47D-XiHL6O-gDKcn8_w\n",
      "ðŸ“˜ Answer: The provided text is about Gradient Boosting for Binary Classification and doesn't mention PCA (Principal Component Analysis). Therefore, I cannot answer your question based on this context.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# Load GCP environment (for Gemini)\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "print(\"ðŸ”‘ Loaded Gemini key:\", google_api_key)\n",
    "\n",
    "# 1. Load PDF and extract text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# 2. Split text into chunks\n",
    "def chunk_text(text, chunk_size=1000, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.create_documents([text])\n",
    "\n",
    "# 3. Create vector store\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "def create_vector_store(documents):\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "\n",
    "    # Use SentenceTransformer directly\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "    # Build FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Store documents alongside their embeddings for lookup\n",
    "    return {\"index\": index, \"documents\": documents, \"model\": model}\n",
    "\n",
    "\n",
    "# 4. Build RAG QA chain with Gemini\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=google_api_key)\n",
    "gemini = genai.GenerativeModel(\"models/gemini-2.0-flash\")\n",
    "\n",
    "def build_rag_qa_chain(vector_store):\n",
    "    def qa(query):\n",
    "        query_embedding = vector_store[\"model\"].encode([query])[0]\n",
    "        D, I = vector_store[\"index\"].search(np.array([query_embedding]), k=3)\n",
    "        relevant_docs = [vector_store[\"documents\"][i].page_content for i in I[0]]\n",
    "\n",
    "        prompt = \"Use the following context to answer the question:\\n\\n\"\n",
    "        prompt += \"\\n\\n\".join(relevant_docs)\n",
    "        prompt += f\"\\n\\nQuestion: {query}\"\n",
    "\n",
    "        response = gemini.generate_content(prompt)\n",
    "        return response.text\n",
    "\n",
    "    return qa\n",
    "\n",
    "\n",
    "# === MAIN PIPELINE ===\n",
    "pdf_path = \"/Users/eunicetu/Downloads/MSDS/Spring 2025/MSDS630/Notes/4_grad_boosting_notes.pdf\"\n",
    "\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "documents = chunk_text(text)\n",
    "vector_store = create_vector_store(documents)\n",
    "qa_chain = build_rag_qa_chain(vector_store)\n",
    "\n",
    "# === USAGE ===\n",
    "qa_chain = build_rag_qa_chain(vector_store)\n",
    "query = \"What's PCA?\"\n",
    "response = qa_chain(query)\n",
    "print(\"Answer:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The general formula for the pseudo-residual is:\n",
      "\n",
      "ri = âˆ’ [âˆ‚L(y, f) / âˆ‚f] evaluated at f = fmâˆ’1(xi), y = yi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is the general formula for pseudo-residuals?\"\n",
    "response1 = qa_chain(query1)\n",
    "print(\"Answer:\", response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The pseudo-residual, denoted as `ri`, is calculated using the negative gradient of the loss function `L(y, f)` with respect to the prediction `f`, evaluated at the previous iteration's prediction `fm-1(xi)` and the actual target `yi`.\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "1. **Loss Function:** The loss function used is the log loss for binary classification: `L(y, f) = log(1 + e-yf(x))`. This function measures the difference between the predicted probability and the actual label.\n",
      "\n",
      "2. **Partial Derivative:** The partial derivative of the loss function with respect to the prediction `f` is calculated as: `âˆ‚L(y, f) / âˆ‚f = -y / (1 + eyf)`.  This represents how much the loss function changes with a small change in the prediction.\n",
      "\n",
      "3. **Pseudo-Residual Formula:** The pseudo-residual is then defined as the negative of this partial derivative, evaluated at the previous model's prediction and the true label:\n",
      "\n",
      "   `ri = - [âˆ‚L(y, f) / âˆ‚f]f=fm-1(xi), y=yi = yi / (1 + eyi fm-1(xi))`\n",
      "\n",
      "In essence, the pseudo-residual tells us the direction and magnitude of the error that the current model `fm-1(x)` is making for the i-th data point.  It represents the amount we need to adjust the prediction to reduce the loss. The sign of `yi` is essential here. When `yi = 1`, `ri` is positive if `fm-1(xi)` is too small (underpredicting), and smaller/closer to zero as `fm-1(xi)` gets closer to the correct value.  When `yi = -1`, `ri` is negative if `fm-1(xi)` is too big (overpredicting).\n",
      "\n",
      "Therefore, the formula for the pseudo-residual,  `ri = yi / (1 + e^(yi * fm-1(xi)))`, quantifies the error between the current model's prediction and the true target, guiding subsequent iterations of the gradient boosting algorithm to improve the model by focusing on data points with larger residuals.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query2 = \"Can you explain the formula for pseudo-residuals?\"\n",
    "response2 = qa_chain(query2)\n",
    "print(\"Answer:\", response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: This document describes Gradient Boosting for Binary Classification. It includes derivations for the initial prediction f0, the formula for pseudo-residuals, and the method for finding the best constant per region in each boosting iteration. There is nothing about the compound interest.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query3 = \"Can you tell me what is compound interest?\"\n",
    "response3 = qa_chain(query3)\n",
    "print(\"Answer:\", response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The provided text describes Gradient Boosting for Binary Classification and doesn't explicitly define linear regression. However, it uses regression trees as a component within the gradient boosting algorithm.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query4 = \"Can you tell me what is linear regression?\"\n",
    "response4 = qa_chain(query4)\n",
    "print(\"Answer:\", response4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/gemini-2.0-flash-live-001\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyBjNSgnILHbnsLM47D-XiHL6O-gDKcn8_w\")\n",
    "\n",
    "models = genai.list_models()\n",
    "for m in models:\n",
    "    print(m.name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
